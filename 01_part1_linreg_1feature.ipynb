{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa5fc13",
   "metadata": {},
   "source": [
    "# Part 1: Linear Regression with One Feature\n",
    "\n",
    "## Stellar Luminosity as a Function of Mass\n",
    "\n",
    "**Objective:** Model stellar luminosity L as a function of stellar mass M using linear regression:\n",
    "$$\\hat{L} = wM + b$$\n",
    "\n",
    "where:\n",
    "- $w$ is the weight\n",
    "- $b$ is the bais\n",
    "- $M$ is stellar mass in solar masses\n",
    "- $L$ is luminosity in solar luminosities\n",
    "\n",
    "We implement everything from first principles without using ML libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d8eaa7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this once if needed)\n",
    "%pip install numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcdeca0",
   "metadata": {},
   "source": [
    "## 1. Dataset Definition and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Stellar mass (M) and luminosity (L)\n",
    "M = np.array([0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "L = np.array([0.15, 0.35, 1.00, 2.30, 4.10, 7.00, 11.2, 17.5, 25.0, 35.0])\n",
    "\n",
    "print(f\"Dataset size: {len(M)} stellar observations\")\n",
    "print(f\"Mass range: {M.min():.1f} - {M.max():.1f} M☉\")\n",
    "print(f\"Luminosity range: {L.min():.2f} - {L.max():.2f} L☉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "plt.figure()\n",
    "plt.scatter(M, L, color='blue', label='Data Points')\n",
    "plt.xlabel('Mass (M☉)')\n",
    "plt.ylabel('Luminosity (L☉)')\n",
    "plt.title('Stellar Mass vs Luminosity')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Print dataset information\n",
    "print('Dataset shape:', M.shape)\n",
    "print('Number of samples:', len(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc83d6",
   "metadata": {},
   "source": [
    "### Commentary on Linearity\n",
    "The relationship between mass and luminosity is clearly non linear.\n",
    "The luminosity increases dramatically as mass increases, suggesting an exponential or power-law relationship rather than linear. This is consistent with astrophysical theory: for main-sequence stars, luminosity approximately follows $L \\propto M^{3}$ to $M^4$.\n",
    "\n",
    "A linear model will therefore have systematic errors, underestimating luminosity at both low and high masses while overestimating in the middle range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9604bb4",
   "metadata": {},
   "source": [
    "## 2. Model Definition and Loss Fuction\n",
    "\n",
    "### Hypothesis Funcion\n",
    "$$h(M, w, b) = wM + b$$\n",
    "\n",
    "### Mean Squared Error Loss\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(M^{(i)}, w, b) - L^{(i)})^2$$\n",
    "\n",
    "where $m$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fed705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Hypothesis function: L_hat = w * M + b\n",
    "def predict(M, w, b):\n",
    "    return w * M + b\n",
    "\n",
    "# Mean Squared Error computation\n",
    "def compute_mse(M, L, w, b):\n",
    "    m = len(M)\n",
    "    L_hat = predict(M, w, b)\n",
    "    errors = L_hat - L\n",
    "    mse = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    return mse\n",
    "\n",
    "#  Test with initial parameters\n",
    "test_w, test_b = 0.0, 0.0\n",
    "\n",
    "test_predictions = predict(M, test_w, test_b)\n",
    "test_loss = compute_mse(M, L, test_w, test_b)\n",
    "\n",
    "print(f\"Initial MSE with w={test_w}, b={test_b}:\")\n",
    "print(f\"Test predictions (first 3): {test_predictions[:3]}\")\n",
    "print(f\"Test MSE loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f7fbc",
   "metadata": {},
   "source": [
    "## 3. Cost Surface Visualization (Mandatory)\n",
    "\n",
    "We evaluate $J(w, b)$ on a grid to understand the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid for w and b\n",
    "w_vals = np.linspace(-5, 25, 100)\n",
    "b_vals = np.linspace(-10, 10, 100)\n",
    "W_grid, B_grid = np.meshgrid(w_vals, b_vals)\n",
    "\n",
    "# Compute cost for each (w, b) pair\n",
    "J_grid = np.zeros_like(W_grid)\n",
    "\n",
    "for i in range(W_grid.shape[0]):\n",
    "    for j in range(W_grid.shape[1]):\\\n",
    "        J_grid[i, j] = compute_mse(M, L, W_grid[i, j], B_grid[i, j])\n",
    "\n",
    "print(f\"Cost surface computed over {W_grid.shape[0]} x {W_grid.shape[1]} grid\")\n",
    "print(f\"Minimum cost on grid: {J_grid.min():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Surface plot of the cost function\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W_grid, B_grid, J_grid, cmap='viridis', alpha=0.8)\n",
    "ax.set_xlabel('Weight (w)')\n",
    "ax.set_ylabel('Bias (b)')\n",
    "ax.set_zlabel('Cost J(w, b)')\n",
    "ax.set_title('Cost Function Surface')\n",
    "plt.show()\n",
    "\n",
    "# Contour plot\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "contour = plt.contour(W_grid, B_grid, J_grid, levels=20,cmap='viridis')\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.xlabel('Weight (w)')\n",
    "plt.ylabel('Bias (b)')\n",
    "plt.title('Cost Function Contours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339eaf8",
   "metadata": {},
   "source": [
    "### Cost Surface Interpretation\n",
    "The minimum of the cost function $J(w,b)$ represents the optimal parameters $(w, b)$ that minimize the prediction error and correspond to the best linear fit of the data in the least-squares sense; furthermore, the convex, \"bowl-shaped\" form of the cost surface confirms the existence of a unique global minimum, which guarantees that the gradient descent algorithm will converge to this optimal solution regardless of the starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d8eb6f",
   "metadata": {},
   "source": [
    "## 4. Gradient Derivation and Implementation \n",
    "\n",
    "### Mathematical Derivation\n",
    "Given:\n",
    "- Hypothesis: $\\hat{L}_i = w \\cdot M^{(i)} + b$\n",
    "- Loss: $J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{L}^{(i)} - {L}^{(i)})^2$\n",
    "Gradients:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{L}^{(i)} - L^{(i)}) \\cdot M^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{L}^{(i)} - L^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ef4254",
   "metadata": {},
   "source": [
    "## 5. Gradient descent (non-vectorized)\n",
    "\n",
    "Task 5: Non-vectorized gradient computation using explicit loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22665d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_loop(M, L, w, b):\n",
    "    m = len(M)\n",
    "    dJ_dw = 0.0\n",
    "    dJ_db = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        L_hat_i = w * M[i] + b\n",
    "        error_i = L_hat_i - L[i]\n",
    "        dJ_dw += error_i * M[i]\n",
    "        dJ_db += error_i\n",
    "    \n",
    "    dJ_dw /= m\n",
    "    dJ_db /= m\n",
    "    return dJ_dw, dJ_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee9054",
   "metadata": {},
   "source": [
    "## 6. Gradient descent (vectorized)\n",
    "\n",
    "Vectorized gradient computation (no sample loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd60412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_vectorized(M, L, w, b):\n",
    "    m = len(M)\n",
    "    L_hat = w * M + b\n",
    "    errors = L_hat - L\n",
    "    \n",
    "    dJ_dw = (1 / m) * np.sum(errors * M)\n",
    "    dJ_db = (1 / m) * np.sum(errors)\n",
    "    return dJ_dw, dJ_db\n",
    "\n",
    "# Verify both implementations match\n",
    "w_test, b_test = 5.0, -2.0\n",
    "dw_loop, db_loop = compute_gradients_loop(M, L, w_test, b_test)\n",
    "dw_vec, db_vec = compute_gradients_vectorized(M, L, w_test, b_test)\n",
    "\n",
    "print(\"Gradient Verification:\")\n",
    "print(f\"Loop version: dJ/dw = {dw_loop:.6f}, dJ/db = {db_loop:.6f}\")\n",
    "print(f\"Vectorized version: dJ/dw = {dw_vec:.6f}, dJ/db = {db_vec:.6f}\")\n",
    "print(f\"Match: {np.allclose([dw_loop, db_loop], [dw_vec, db_vec])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a92b2",
   "metadata": {},
   "source": [
    "### Gradient Descent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(M, L, w_init, b_init, alpha, iterations, vectorized=True):\n",
    "    \"\"\"Run gradient descent to optimize w and b\"\"\"\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    cost_history = []\n",
    "    \n",
    "    grad_func = compute_gradients_vectorized if vectorized else compute_gradients_loop\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        dJ_dw, dJ_db = grad_func(M, L, w, b)\n",
    "        w = w - alpha * dJ_dw\n",
    "        b = b - alpha * dJ_db\n",
    "        \n",
    "        cost = compute_mse(M, L, w, b)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i % (iterations // 10) == 0:\n",
    "            print(f\"Iter {i:4d}: w={w:8.4f}, b={b:8.4f}, J={cost:10.4f}\")\n",
    "    \n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ad3c6",
   "metadata": {},
   "source": [
    "## 7. Experiments with Different Learning Rates (MANDATORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.05]\n",
    "iterations = 5000\n",
    "w_init, b_init = 0.0, 0.0\n",
    "\n",
    "results = {}\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with learning rate α = {alpha}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    w_final, b_final, cost_hist = gradient_descent(\n",
    "        M, L, w_init, b_init, alpha, iterations, vectorized=True\n",
    "    )\n",
    "    \n",
    "    results[alpha] = {\n",
    "        'w': w_final,\n",
    "        'b': b_final,\n",
    "        'final_loss': cost_hist[-1],\n",
    "        'history': cost_hist\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"w = {w_final:.6f}\")\n",
    "    print(f\"b = {b_final:.6f}\")\n",
    "    print(f\"Final Loss = {cost_hist[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475e586",
   "metadata": {},
   "source": [
    "## 8. Convergence Analysis (MANDATORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40516a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Full convergence plot\n",
    "plt.subplot(1, 2, 1)\n",
    "for alpha, data in results.items():\n",
    "    plt.plot(data['history'], label=f'α = {alpha}', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(w,b)')\n",
    "plt.title('Convergence: Loss vs Iterations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Zoomed view (skip first iterations)\n",
    "plt.subplot(1, 2, 2)\n",
    "skip = 100\n",
    "for alpha, data in results.items():\n",
    "    plt.plot(range(skip, len(data['history'])), \n",
    "             data['history'][skip:], label=f'α = {alpha}', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost J(w,b)')\n",
    "plt.title(f'Convergence (after {skip} iterations)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26025899",
   "metadata": {},
   "source": [
    "### Convergence Analysis\n",
    "\n",
    "convergence speed:\n",
    "- α = 0.001: slow - takes many iterations to converge\n",
    "- α = 0.01:  moderate - good balance of speed and stability\n",
    "- α = 0.05:  fast - rapid initial decrease, converges quickly\n",
    "\n",
    "stability:\n",
    "- all learning rates are stable (no divergence)\n",
    "- cost decreases monotonically for all α values\n",
    "- convex optimization ensures guaranteed convergence\n",
    "\n",
    "optimal choice:\n",
    "- α = 0.01 or 0.05 recommended for this problem\n",
    "- higher α possible without instability due to small dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01c344",
   "metadata": {},
   "source": [
    "## 9. Final Fit Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837ff40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ad574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model (lr = 0.01)\n",
    "best_alpha = 0.01\n",
    "w_best = results[best_alpha]['w']\n",
    "b_best = results[best_alpha]['b']\n",
    "print(f\"Best model parameters (α={best_alpha}): w = {w_best:.6f}, b = {b_best:.6f}\")\n",
    "\n",
    "# Generate predictions using the best model\n",
    "L_pred = predict(M, w_best, b_best)\n",
    "M_plot = np.linspace(M.min(), M.max(), 100)\n",
    "L_pred_plot = predict(M_plot, w_best, b_best)\n",
    "\n",
    "# Fit plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(M, L, color='blue', label='Data Points')\n",
    "plt.plot(M_plot, L_pred_plot, color='red', label='Best Fit Line', linewidth=2)\n",
    "plt.xlabel('Mass (M☉)')\n",
    "plt.ylabel('Luminosity (L☉)')\n",
    "plt.title('Stellar Mass vs Luminosity with Best Fit')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b79fd0",
   "metadata": {},
   "source": [
    "### Systematic Error Discussion\n",
    "\n",
    "The residual plot reveals clear systematic errors in the linear model:\n",
    "\n",
    "- Underestimation at extremes: The model underestimates luminosity for both low-mass stars $M < 0.8$ and high-mass stars $M > 2.0$.\n",
    "\n",
    "- Overestimation in middle: The model overestimates luminosity for intermediate-mass stars $M ≈ 1.0-1.5$.\n",
    "\n",
    "- Physical insight: This systematic pattern confirms that the mass-luminosity relationship is nonlinear, following a power law $L \\propto M^{3.5}$, rather than a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8df4e",
   "metadata": {},
   "source": [
    "## 10. Conceptual Questions\n",
    "\n",
    "### Q1: Astrophysical Meaning of w\n",
    "\n",
    "The parameter $w$ represents the rate of change of luminosity with respect to mass in our linear approximation. In physical terms:\n",
    "\n",
    "- Units: w has units of $L☉/M☉$ (solar luminosities per solar mass)\n",
    "- Interpretation: For each additional solar mass, the luminosity increases by approximately w solar luminosities\n",
    "- Value: Our fitted $w ≈ 19.5$ suggests that in the linear approximation, luminosity increases by about $19.5$ $L☉$ for each $1$ $M☉$ increase in mass\n",
    "\n",
    "However, this linear interpretation oversimplifies the true relationship, which is actually a power law.\n",
    "\n",
    "### Q2: Why is a Linear Model Limited?\n",
    "\n",
    "A linear model is fundamentally limited for stellar luminosity because:\n",
    "\n",
    "1. Physical theory: The mass-luminosity relation for main-sequence stars follows $L \\propto M^{α}$ where $α ≈ 3.5-4.0$, not $L \\propto M$\n",
    "\n",
    "2. Nuclear fusion physics: Luminosity depends on the core temperature and pressure, which scale nonlinearly with mass\n",
    "\n",
    "3. Observed systematic errors: The U-shaped residual pattern demonstrates the model cannot capture the accelerating increase in luminosity with mass\n",
    "\n",
    "4. Limited predictive power: The linear model will perform poorly for extrapolation beyond the training range\n",
    "\n",
    "Solution: Polynomial or logarithmic transformations can better capture this nonlinear relationship, which we'll explore in Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2dc07",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "A linear model is structurally incapable of representing the true mass-luminosity relationship. The limitation is not in the optimization algorithm (gradient descent works perfectly) but in the expressive power of the linear function class. This is a fundamental lesson in machine learning: model selection matters as much as optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
